{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, DateTime, Float, select, Date, JSON\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "from pandas.util.testing import assert_frame_equal\n",
    "\n",
    "from wikibaseintegrator import wbi_core, wbi_login, wbi_login\n",
    "from wikibaseintegrator.wbi_config import config as wbi_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRICKIT_BD_LOGIN = 'postgres'\n",
    "BRICKIT_BD_PASSWORD = 'pass'\n",
    "BRICKIT_BD_HOST = 'host'\n",
    "WIKIBASE_HOST = '84.201.142.182'\n",
    "\n",
    "WIKIBASE_LOGIN = 'WikibaseAdmin'\n",
    "WIKIBASE_PASSWORD = 'WikibaseDockerAdminPass'\n",
    "\n",
    "connection_string = f'postgresql://{BRICKIT_BD_LOGIN}:{BRICKIT_BD_PASSWORD}@{BRICKIT_BD_HOST}:5432/holybricks'\n",
    "engine_pg = create_engine(connection_string)\n",
    "\n",
    "wbi_config['MEDIAWIKI_API_URL'] = f'http://{WIKIBASE_HOST}:8181/api.php'\n",
    "wbi_config['SPARQL_ENDPOINT_URL'] = f'http://{WIKIBASE_HOST}:8989/bigdata/sparql'\n",
    "wbi_config['WIKIBASE_URL'] = 'http://wikibase.svc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Легаси, уже умеем нормально\n",
    "ITEMS_DICT = {\n",
    "    'I': {\n",
    "        'Brickit Company': 'Q1',\n",
    "        'Brickit Image': 'Q2',\n",
    "        'Brickit Part': 'Q3'\n",
    "\n",
    "    },\n",
    "    'P': {\n",
    "        'instance of': 'P1',\n",
    "        'Image URL': 'P2',\n",
    "        'Image ID': 'P3',\n",
    "        'Part Name': 'P4',\n",
    "        'Part Tag': 'P5',\n",
    "        'Part Image': 'P6',\n",
    "        'Part Num': 'P7',\n",
    "        'Part Child': 'P8'\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Забрать какие-то данные из Brickit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_sql(f\"\"\"\n",
    "SELECT id, public_url\n",
    "FROM staging.manual_images\n",
    "WHERE initial_entity_type = 'part'\n",
    "    \"\"\", con = engine_pg)\n",
    "\n",
    "\n",
    "df_parts = pd.read_sql(f\"\"\"\n",
    "SELECT part_num, \"name\", tag, part_cat_id, child_part_nums, image_id\n",
    "FROM staging.synthetic_parts\n",
    "    \"\"\", con = engine_pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Залить сущности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Костыль для связки наша сущность - ID в Wikibase/\n",
    "# Предполагаем, что это можно взять при помощи SPARQL\n",
    "# А пока что по результатам циклов ниже в эти датафреймы дописывается ID Wikibase и сохраняются в csv\n",
    "# df_images = pd.read_csv('./df_images.csv')\n",
    "# df_parts = pd.read_csv('./df_parts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_instance = wbi_login.Login(user=WIKIBASE_LOGIN, pwd=WIKIBASE_PASSWORD)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить 100 изображений из всех\n",
    "for _, img_i in df_images[:100].iterrows():\n",
    "    data = [\n",
    "        wbi_core.Url(str(img_i['public_url']), prop_nr=ITEMS_DICT['P']['Image URL']),\n",
    "        wbi_core.String(str(img_i['id']), prop_nr=ITEMS_DICT['P']['Image ID']),\n",
    "        wbi_core.ItemID(ITEMS_DICT['I']['Brickit Image'], prop_nr=ITEMS_DICT['P']['instance of'])\n",
    "    ]\n",
    "    item = wbi_core.ItemEngine(new_item=True, data=data,core_props=set())\n",
    "    \n",
    "    # Этот метод в библиотеке из коробки не работает. \n",
    "    # Надо либо закомментить в библиотеке в wbi_core.ItemEngine.set_label() условие после \"Skip set_label if the item already have one and if_exists is at 'KEEP'\"\n",
    "    # Либо просто не проставлять лейблы\n",
    "    item.set_label('img_' + str(img_i['id']), if_exists='REPLACE')\n",
    "    \n",
    "    r = item.write(login_instance)    \n",
    "    \n",
    "    df_images.loc[df_images.id == img_i['id'], 'entity_id'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.to_csv('./df_images.csv', index = False) #Обновляем \"базу знаний\" про связки с id Wikibase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Заливка деталей\n",
    "df_parts_img = df_parts[df_parts.image_id.isin(df_images[~df_images.entity_id.isnull()].id)] #только тех, для которых мы уже залили картинки\n",
    "for _, part_i in df_parts_img.iterrows():\n",
    "    part_img = part_i['image_id']\n",
    "    img_Q = df_images[df_images.id == part_img].reset_index().at[0, 'entity_id']\n",
    "    \n",
    "    data = [\n",
    "        wbi_core.String(str(part_i['name']), prop_nr=ITEMS_DICT['P']['Part Name']),\n",
    "        wbi_core.String(str(part_i['tag']), prop_nr=ITEMS_DICT['P']['Part Tag']),\n",
    "        wbi_core.String(str(part_i['part_num']), prop_nr=ITEMS_DICT['P']['Part Num']),\n",
    "        wbi_core.ItemID(ITEMS_DICT['I']['Brickit Part'], prop_nr=ITEMS_DICT['P']['instance of']),\n",
    "        wbi_core.ItemID(img_Q, prop_nr=ITEMS_DICT['P']['Part Image'])\n",
    "    ]\n",
    "    item = wbi_core.ItemEngine(new_item=True, data=data,core_props=set())\n",
    "    \n",
    "    # Этот метод в библиотеке из коробки не работает. \n",
    "    item.set_label('part_num_' + str(part_i['part_num']), if_exists='REPLACE')\n",
    "    \n",
    "    r = item.write(login_instance)    \n",
    "    \n",
    "    df_parts.loc[df_parts.part_num == part_i['part_num'], 'entity_id'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parts.to_csv('./df_parts.csv', index = False) #Обновляем \"базу знаний\" про связки с id Wikibase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эвотор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгрузка схемы базы из Эвотора\n",
    "'''\n",
    "select col.column_id, \n",
    "       col.owner as schema_name,\n",
    "       col.table_name, \n",
    "       col.column_name, \n",
    "       col.data_type, \n",
    "       col.data_length, \n",
    "       col.data_precision, \n",
    "       col.data_scale, \n",
    "       col.nullable\n",
    "from sys.all_tab_columns col\n",
    "inner join sys.all_tables t on col.owner = t.owner \n",
    "                              and col.table_name = t.table_name\n",
    "-- excluding some Oracle maintained schemas\n",
    "where col.owner not in ('ANONYMOUS','CTXSYS','DBSNMP','EXFSYS', 'LBACSYS', \n",
    "   'MDSYS', 'MGMT_VIEW','OLAPSYS','OWBSYS','ORDPLUGINS', 'ORDSYS','OUTLN', \n",
    "   'SI_INFORMTN_SCHEMA','SYS','SYSMAN','SYSTEM','TSMSYS','WK_TEST','WKSYS', \n",
    "   'WKPROXY','WMSYS','XDB','APEX_040000', 'APEX_PUBLIC_USER','DIP', \n",
    "   'FLOWS_30000','FLOWS_FILES','MDDATA', 'ORACLE_OCM', 'XS$NULL',\n",
    "   'SPATIAL_CSW_ADMIN_USR', 'SPATIAL_WFS_ADMIN_USR', 'PUBLIC')  \n",
    "order by col.owner, col.table_name, col.column_id;\n",
    "'''\n",
    "\n",
    "df_e = pd.read_csv('./evotor_schemas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмём самые ходовые схемы для тестов\n",
    "schema_list = [\n",
    "    'AIRFLOW', \n",
    "    'BIGDATA_LOADER', \n",
    "    'EVOTOR_ANALYTICS', \n",
    "    'EVOTOR_BIGDATA', \n",
    "    'EVOTOR_MARKET_REPL',\n",
    "    'EVOTOR_REPORTS',\n",
    "    'EVOTOR_CRM'\n",
    "]\n",
    "df_e = df_e[df_e.SCHEMA_NAME.isin(schema_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какие properties будут нужны \n",
    "prop_list = ['Field', 'Description', 'Schema','Table','located in','Data Type','Data Length']\n",
    "prop_df = get_items_by_label(prop_list, item_type = 'P')\n",
    "prop_df = {i['label']: i['item'] for _, i in prop_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание схем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_database = get_items_by_label(['dwh'], item_type = 'Q').at[0, 'item']\n",
    "for schema in schema_list:\n",
    "    data = [\n",
    "        wbi_core.ItemID(Q_database, prop_nr=prop_df['located in'])\n",
    "    ]\n",
    "    item = wbi_core.ItemEngine(new_item=True, data=data,core_props=set())\n",
    "\n",
    "    item.set_label(schema, if_exists='REPLACE')\n",
    "\n",
    "    r = item.write(login_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for schema in tqdm(schema_list):\n",
    "    print(schema)\n",
    "    Q_schema = get_items_by_label([schema], item_type = 'Q').at[0, 'item'] # ID объекта DWH\n",
    "    df_schema = df_e[df_e.SCHEMA_NAME == schema]\n",
    "    \n",
    "    # Таблицы\n",
    "    for table in tqdm(df_schema.TABLE_NAME.unique()):\n",
    "        if '#' in table:\n",
    "            continue\n",
    "        \n",
    "        df_table = df_schema[df_schema.TABLE_NAME == table]\n",
    "        \n",
    "        fields = []\n",
    "        # Квалифаеры\n",
    "        for _, field in df_table.iterrows(): \n",
    "            qualifiers = [\n",
    "                wbi_core.String(field['DATA_TYPE'], prop_nr=prop_df['Data Type'], is_qualifier = True),\n",
    "                wbi_core.String(str(field['DATA_LENGTH']), prop_nr=prop_df['Data Length'], is_qualifier = True),\n",
    "            ]\n",
    "            # Поля\n",
    "            fields.append(wbi_core.String(field['COLUMN_NAME'], prop_nr=prop_df['Field'], qualifiers=qualifiers))\n",
    "                \n",
    "        data = [wbi_core.ItemID(Q_schema, prop_nr=prop_df['located in'])]\n",
    "        data.extend(fields)\n",
    "        \n",
    "        item = wbi_core.ItemEngine(new_item=True, data=data,core_props=set())\n",
    "\n",
    "        item.set_label(schema + '.' + table, if_exists='REPLACE')\n",
    "\n",
    "        r = item.write(login_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перечислить таблицы в схемах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for schema in schema_list:\n",
    "    print(schema)\n",
    "    Q_schema = get_items_by_label([schema], item_type = 'Q').at[0, 'item'] # ID объекта DWH\n",
    "\n",
    "    df_schema = df_e[(df_e.SCHEMA_NAME == schema) & (~df_e.TABLE_NAME.str.contains('#'))]\n",
    "    df_schema['lables'] = df_schema.SCHEMA_NAME + '.' + df_schema.TABLE_NAME\n",
    "    \n",
    "    lables_list = list(set(df_schema.lables))\n",
    "    \n",
    "    batch = 25\n",
    "    Q_tables = []\n",
    "    for i in tqdm(range(ceil(len(lables_list) / batch)), desc = 'batches'):\n",
    "        lables_list_i = lables_list[batch*i : batch*(i+1)]\n",
    "        Q_tables_i = get_items_by_label(lables_list_i, item_type = 'Q').item.to_list()\n",
    "        Q_tables.extend(Q_tables_i)\n",
    "        \n",
    "    data = [wbi_core.ItemID(Q_i, prop_nr=prop_df['Table']) for Q_i in Q_tables]\n",
    "\n",
    "    item = wbi_core.ItemEngine(new_item=False, item_id = Q_schema, data=data,core_props=set())\n",
    "\n",
    "    item.write(login_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_by_label(label_list:list, item_type:str, is_unique:bool = True, is_notnull:bool = True):\n",
    "    '''\n",
    "    По переданному списку лейблов находит entity_id в базе Wikibase. \n",
    "    \n",
    "    label_list: список искомых лейблов\n",
    "    item_type: тип искомого объекта. Если не указано, то любой объект. Если указано:\n",
    "        \"P\" - Property\n",
    "        \"Q\" - Item\n",
    "    is_unique: если True, то вернёт ошибку, если найдено больше одного значения\n",
    "    is_notnull: если True, то вернёт ошибку, если не найдено ни одного значения\n",
    "    '''\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "          ?item rdfs:label ?itemLabel. \n",
    "\n",
    "          VALUES ?itemLabel {{ {label_filter} }}\n",
    "        }}\"\"\".format(label_filter = ' '.join([f'\\\"{i}\\\"@en' for i in label_list]))\n",
    "    \n",
    "    result = wbi_core.ItemEngine.execute_sparql_query(query)\n",
    "    result_list = [[i['itemLabel']['value'], i['item']['value'].replace('http://wikibase.svc/entity/', '')] \n",
    "                   for i in result['results']['bindings']]\n",
    "    \n",
    "    df = pd.DataFrame(result_list, columns = ['label', 'item'])\n",
    "        \n",
    "    if item_type in ('P', 'Q'):\n",
    "        df = df[df.item.str.contains(item_type)] \n",
    "\n",
    "    df_check = df.groupby('label').count()\n",
    "    if is_unique and df_check.item.max() > 1:\n",
    "        r = df[df.label.isin(df_check[df_check.item > 1].index.to_list())].sort_values(by = 'label')\n",
    "        logging.info(f\"entity_id определён неоднозначно: \\n{r}\")\n",
    "        return None\n",
    "    elif is_notnull and len(set(label_list) - set(df.label)) > 0:\n",
    "        r = set(label_list) - set(df.label)\n",
    "        logging.info(f\"entity_id не найден: \\n{r}!\")\n",
    "        return None\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_instance = wbi_login.Login(user=WIKIBASE_LOGIN, pwd=WIKIBASE_PASSWORD)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDatabase():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def fetch_schemas()\n",
    "        return self.resolved_schemas \n",
    "    \n",
    "    def push_to_wb(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiSchema():    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def fetch_tables()\n",
    "        return self.resolved_tables \n",
    "    \n",
    "    def push_to_wb(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTable():\n",
    "    @staticmethod\n",
    "    def get_wb_schema(Q:str, properties_df:pd.DataFrame, login_instance:wbi_login.Login) -> str:\n",
    "        '''\n",
    "        Вернуть entity_id схемы, в которой находится таблица с entity_id = Q\n",
    "        '''\n",
    "        \n",
    "        query = '''\n",
    "            SELECT ?schema ?schema_name WHERE {{\n",
    "                ?schema wdt:{table} wd:{Q} .\n",
    "                ?schema rdfs:label ?schema_name .\n",
    "            }}'''.format(\n",
    "                Q = Q,\n",
    "                table = properties_df['Table'],        \n",
    "            )\n",
    "        result = wbi_core.ItemEngine.execute_sparql_query(query)\n",
    "        \n",
    "        result_list = [[i['schema_name']['value'], i['schema']['value'].replace('http://wikibase.svc/entity/', '')]\n",
    "                       for i in result['results']['bindings']]\n",
    "\n",
    "        result_df = pd.DataFrame(result_list, columns = ['schema_name', 'schema'])\n",
    "        \n",
    "        if result_df.shape[0] > 1:\n",
    "            raise Exception(f'Table with entity_id {Q} finded in several schemas: \\n{result_df.shema.to_list()}')\n",
    "        elif result_df.shape[0] == 0:\n",
    "            raise Exception(f'Table with entity_id {Q} not finded in schemas!')\n",
    "        else:\n",
    "            return (result_df.at[0, 'schema_name'], result_df.at[0, 'schema'])\n",
    "    \n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_wb_fields(Q:str, properties_df:pd.DataFrame, login_instance:wbi_login.Login) -> pd.DataFrame:\n",
    "        query = '''\n",
    "            SELECT  ?COLUMN_NAME ?DATA_TYPE ?DATA_LENGTH ?DESCRIPTION\n",
    "            WHERE\n",
    "            {{\n",
    "                 wd:{Q} p:{field} ?statement.\n",
    "                 ?statement ps:{field} ?COLUMN_NAME.\n",
    "                 \n",
    "                 # CORE FIELDS\n",
    "                 ?statement pq:{data_type} ?DATA_TYPE.\n",
    "                 ?statement pq:{data_length} ?DATA_LENGTH.\n",
    "                 \n",
    "                 # CUSTOM FIELD\n",
    "                 OPTIONAL {{?statement pq:{descrition} ?DESCRIPTION.}}  \n",
    "            }}        \n",
    "        '''.format(\n",
    "            Q = Q,\n",
    "            field = properties_df['Field'],\n",
    "            data_type = properties_df['Data Type'],\n",
    "            data_length = properties_df['Data Length'],\n",
    "            descrition = properties_df['Description'] \n",
    "        )   \n",
    "        \n",
    "        result = wbi_core.ItemEngine.execute_sparql_query(query)\n",
    "\n",
    "        wb_fields_df = []\n",
    "        for bind in result['results']['bindings']:\n",
    "            wb_fields_df.append({k: v['value'] for k, v in bind.items()})\n",
    "        wb_fields_df = pd.DataFrame(wb_fields_df)        \n",
    "        \n",
    "        return wb_fields_df\n",
    "    \n",
    "\n",
    "    def __fetch_fields(self):        \n",
    "        if self.new_item:\n",
    "            self.resolved_fields = df_input.copy()\n",
    "        elif self.df_input.shape[0] == 0:\n",
    "            logging.info('No input dataframe, can`t fetch!')\n",
    "        else:\n",
    "            try: \n",
    "                assert_frame_equal(\n",
    "                    self.df_input[['COLUMN_NAME', 'DATA_TYPE', 'DATA_LENGTH']].astype(str), \n",
    "                    self.wb_fields[['COLUMN_NAME', 'DATA_TYPE', 'DATA_LENGTH']].astype(str), \n",
    "                    check_like = False\n",
    "                )\n",
    "                logging.info(f'Table {self.name} has no changes, skipping update.')\n",
    "            except:\n",
    "                logging.info(\"\"\"\n",
    "                    {columns} columns in new table\n",
    "                    {added} colums are added\n",
    "                    {deleted} columns are deleted              \n",
    "                \"\"\".format(\n",
    "                    columns = self.df_input.shape[0],\n",
    "                    added = self.df_input[~self.df_input.COLUMN_NAME.isin(self.wb_fields.COLUMN_NAME)].shape[0],\n",
    "                    deleted = self.wb_fields[~self.wb_fields.COLUMN_NAME.isin(self.df_input.COLUMN_NAME)].shape[0]                    \n",
    "                ))\n",
    "                \n",
    "                self.resolved_fields = df_input.merge(self.wb_fields[['COLUMN_NAME', 'DESCRIPTION']],\n",
    "                                                     how = 'left', left_on = 'COLUMN_NAME', right_on = 'COLUMN_NAME')\n",
    "            \n",
    "            return self.resolved_fields    \n",
    "    \n",
    "    \n",
    "    def __init__(self, name:str, properties_df:pd.DataFrame, login_instance:wbi_login.Login, df_input = pd.DataFrame()):\n",
    "        self.name = name\n",
    "        self.login_instance = login_instance\n",
    "        self.properties_df = properties_df\n",
    "        self.df_input = df_input\n",
    "        \n",
    "        Q_df = get_items_by_label([name], item_type = 'Q')\n",
    "        if Q_df is None:\n",
    "            logging.info(f'No such table: {name}! New one will be created.')\n",
    "            if self.df_input.shape[0] == 0:\n",
    "                raise Exception('Cannot create new item from empty input DataFrame!')\n",
    "            self.new_item = True                      \n",
    "            self.Q = None\n",
    "            schema_name = df_input.SCHEMA_NAME.unique()\n",
    "            if schema_name.shape[0] > 1:\n",
    "                raise Exception('Ambiguous schema_name: {schema_name} ')\n",
    "            else:\n",
    "                self.schema_name = schema_name[0]\n",
    "            \n",
    "            self.Q_schema = get_items_by_label([self.schema_name], item_type = 'Q').at[0, 'item']\n",
    "        else:\n",
    "            self.new_item = False\n",
    "            self.Q = Q_df.at[0, 'item']\n",
    "            self.schema_name, self.Q_schema = self.get_wb_schema(self.Q, self.properties_df, self.login_instance)\n",
    "            self.wb_fields = self.get_wb_fields(self.Q, self.properties_df, self.login_instance)\n",
    "        \n",
    "        self.__fetch_fields()\n",
    "        logging.info(\"\"\"\n",
    "            Table {name} (entity_id: {Q}), schema {schema_name} (entity_id: {Qs})\n",
    "        \"\"\".format(\n",
    "            name = self.name,\n",
    "            Q = self.Q,\n",
    "            schema_name = self.schema_name,\n",
    "            Qs = self.Q_schema\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    def push_to_wiki(self):\n",
    "        # TO DO: проверка на дубликаты полей\n",
    "        # А это можно на SPARQL сделать? Хотя зачем. \n",
    "        self.resolved_fields\n",
    "\n",
    "        fields = []\n",
    "        # Квалифаеры\n",
    "        for _, field in self.resolved_fields.iterrows(): \n",
    "            qualifiers = [\n",
    "                wbi_core.String(field['DATA_TYPE'], prop_nr=prop_df['Data Type'], is_qualifier = True),\n",
    "                wbi_core.String(str(field['DATA_LENGTH']), prop_nr=prop_df['Data Length'], is_qualifier = True),\n",
    "            ]\n",
    "            description_i =  field['DESCRIPTION']\n",
    "            if description_i:\n",
    "                qualifiers.append(\n",
    "                    wbi_core.String(str(description_i), prop_nr=prop_df['Description'], is_qualifier = True),\n",
    "                )\n",
    "            \n",
    "            # Поля\n",
    "            fields.append(wbi_core.String(field['COLUMN_NAME'], prop_nr=prop_df['Field'], qualifiers=qualifiers))\n",
    "\n",
    "        data = [wbi_core.ItemID(self.Q_schema, prop_nr=prop_df['located in'])]\n",
    "        data.extend(fields)\n",
    "\n",
    "        item = wbi_core.ItemEngine(new_item=self.new_item, data=data,core_props=set())\n",
    "\n",
    "        if self.new_item:\n",
    "            item.set_label(schema + '.' + table, if_exists='REPLACE')\n",
    "\n",
    "        r = item.write(login_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:entity_id не найден: \n",
      "{'AIRFLOW.TEST_2'}!\n",
      "INFO:root:No such table: AIRFLOW.TEST_2! New one will be created.\n",
      "INFO:root:\n",
      "            Table AIRFLOW.TEST_2 (entity_id: None), schema AIRFLOW (entity_id: Q203)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "t2 = WikiTable('AIRFLOW.TEST_2', prop_df, login_instance, df_input[['SCHEMA_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'DATA_LENGTH']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "                    1 columns in new table\n",
      "                    0 colums are added\n",
      "                    1 columns are deleted              \n",
      "                \n",
      "INFO:root:\n",
      "            Table AIRFLOW.TEST_1 (entity_id: Q254), schema AIRFLOW (entity_id: Q203)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "t1 = WikiTable('AIRFLOW.TEST_1', prop_df, login_instance, df_input[['SCHEMA_NAME', 'COLUMN_NAME', 'DATA_TYPE', 'DATA_LENGTH']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прочие знания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API поиска\n",
    "wbi_core.ItemEngine.get_search_results('part_num_10793') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дескрипшн\n",
    "set_description(self, description, lang=None, if_exists='REPLACE'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновление существующих айтемов\n",
    "data = [\n",
    "    wbi_core.ItemID(img_Q, prop_nr = ITEMS_DICT['P']['Part Image'])\n",
    "]\n",
    "item = wbi_core.ItemEngine(new_item=False, item_id = 'Q1234', data=data,core_props=set())\n",
    "r = item.write(login_instance)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
